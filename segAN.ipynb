{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/haobo/HaoboSeg-pytorch/eizzaty/.venv/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/haobo/HaoboSeg-pytorch/eizzaty/.venv/lib/python3.8/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/haobo/HaoboSeg-pytorch/eizzaty/.venv/lib/python3.8/site-packages/torchvision/image.so: undefined symbol: _ZN3c104cuda20CUDACachingAllocator9allocatorE'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "from torchmetrics import Dice\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import albumentations as A\n",
    "import torchvision.transforms.functional as TF\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os, time\n",
    "import matplotlib.pyplot as plt\n",
    "from glob import glob\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from path import Path\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else 'cpu')\n",
    "# torch.cuda.set_device(0)\n",
    "\n",
    "# print(device)\n",
    "# print(torch.cuda.current_device())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Size:\n",
      "Train images: 3813\t Train masks: 3813\n",
      "Val images: 195\t Val masks: 195\n"
     ]
    }
   ],
   "source": [
    "LEARNING_RATE= 1e-3\n",
    "BATCH_SIZE= 8\n",
    "NUM_EPOCHS= 10\n",
    "NUM_WORKERS= 0\n",
    "\n",
    "IMAGE_HEIGHT= 512\n",
    "IMAGE_WIDTH= 416\n",
    "PIN_MEMORY= True\n",
    "LOAD_MODEL= False\n",
    "\n",
    "# num_block= [3, 4, 6, 3];\n",
    "features_depth= [64, 128, 256, 512]\n",
    "input_channel= 1 \n",
    "num_classes= 3\n",
    "\n",
    "model_category = 'segAN'\n",
    "checkpoint_path = 'segAN.pth'\n",
    "# training_checkpoint = 'training_checkpoint.pth'\n",
    "\n",
    "TRAIN_IMG_DIR = sorted(glob(\"/home/haobo/HaoboSeg-pytorch/data_all/train_f/*\"))\n",
    "TRAIN_MASK_DIR = sorted(glob(\"/home/haobo/HaoboSeg-pytorch/data_all/train_m/*\"))\n",
    "\n",
    "VAL_IMG_DIR = sorted(glob(\"/home/haobo/HaoboSeg-pytorch/data_all/val_f/*\"))\n",
    "VAL_MASK_DIR = sorted(glob(\"/home/haobo/HaoboSeg-pytorch/data_all/val_m/*\"))\n",
    "\n",
    "data_str = f\"Dataset Size:\\nTrain images: {len(TRAIN_IMG_DIR)}\\t Train masks: {len(TRAIN_MASK_DIR)}\"\n",
    "print(data_str)\n",
    "\n",
    "data_str = f\"Val images: {len(VAL_IMG_DIR)}\\t Val masks: {len(VAL_MASK_DIR)}\"\n",
    "print(data_str)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "477"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class EchoDataset(Dataset):\n",
    "    def __init__(self, images_path, masks_path, transform=None):\n",
    "        self.images_path = images_path\n",
    "        self.masks_path = masks_path\n",
    "        self.transform = transform\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = cv2.imread(self.images_path[index], cv2.IMREAD_GRAYSCALE)\n",
    "        image = cv2.resize(image, (IMAGE_WIDTH, IMAGE_HEIGHT), interpolation=cv2.INTER_NEAREST)\n",
    "        image = image/image.max()\n",
    "        image = np.expand_dims(image, axis=0)\n",
    "        image = image.astype(np.float32)\n",
    "\n",
    "        mask = cv2.imread(self.masks_path[index], cv2.IMREAD_GRAYSCALE)        \n",
    "        mask = cv2.resize(mask, (IMAGE_WIDTH, IMAGE_HEIGHT), interpolation=cv2.INTER_NEAREST)\n",
    "        masks = [(mask==c) for c in range(3)]\n",
    "        mask = np.stack(masks, axis=0)\n",
    "        mask = mask.astype(np.float32)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            augmentation= self.transform(image= image, mask= mask)\n",
    "            image = augmentation['image']\n",
    "            mask = augmentation['mask']\n",
    "\n",
    "            # image = np.transpose(image, (1,2,0)).to(torch.float32)\n",
    "            # mask = mask.to(torch.float32)\n",
    "\n",
    "        return image, mask, self.images_path[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images_path)\n",
    "\n",
    "transform = A.Compose([\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.RandomGamma(gamma_limit= 70,p=0.6)\n",
    "\n",
    "])\n",
    "\n",
    "def get_train_data(train_img_dir, train_mask_dir, val_img_dir, val_mask_dir, batch_size, train_transform, val_transform, num_workers, pin_memory):\n",
    "    train_ds= EchoDataset(train_img_dir, train_mask_dir, train_transform)\n",
    "    train_dataloader= DataLoader(train_ds, batch_size=batch_size,\n",
    "                                 shuffle=True, \n",
    "                                 num_workers=num_workers,\n",
    "                                 pin_memory=pin_memory)\n",
    "    val_ds= EchoDataset(val_img_dir, val_mask_dir, val_transform)\n",
    "    val_dataloader= DataLoader(val_ds, batch_size=batch_size,\n",
    "                               shuffle=False,\n",
    "                               num_workers=num_workers,\n",
    "                               pin_memory=pin_memory)\n",
    "\n",
    "    return train_dataloader, val_dataloader\n",
    "\n",
    "def get_test_data(test_img_dir, test_mask_dir, batch_size, test_transform, num_workers, pin_memory):\n",
    "    test_ds= EchoDataset(test_img_dir, test_mask_dir, test_transform)\n",
    "    test_dataloader= DataLoader(test_ds, batch_size=batch_size,\n",
    "                                shuffle= False,\n",
    "                                num_workers=num_workers,\n",
    "                                pin_memory=pin_memory) \n",
    "    return test_dataloader\n",
    "\n",
    "\n",
    "# train_ds= EchoDataset(TRAIN_IMG_DIR, TRAIN_MASK_DIR, transform)\n",
    "# print(ds[1][0].dtype)\n",
    "train_dataloader, val_dataloader = get_train_data(train_img_dir= TRAIN_IMG_DIR, train_mask_dir= TRAIN_MASK_DIR, \n",
    "                                                  val_img_dir= VAL_IMG_DIR, val_mask_dir= VAL_MASK_DIR, \n",
    "                                                  train_transform=None, val_transform=None,\n",
    "                                                  batch_size= BATCH_SIZE, \n",
    "                                                  num_workers= NUM_WORKERS, \n",
    "                                                  pin_memory= PIN_MEMORY)\n",
    "\n",
    "len(train_dataloader)\n",
    "\n",
    "                        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Call Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "el1: torch.Size([8, 64, 64, 64])\n",
      "el2: torch.Size([8, 128, 32, 32])\n",
      "el3: torch.Size([8, 256, 16, 16])\n",
      "el4: torch.Size([8, 512, 8, 8])\n",
      "dl1: torch.Size([8, 256, 16, 16])\n",
      "dl2: torch.Size([8, 128, 32, 32])\n",
      "dl3: torch.Size([8, 64, 64, 64])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.4390, 0.3773, 0.3539,  ..., 0.4584, 0.3299, 0.4455],\n",
       "          [0.4917, 0.4988, 0.5751,  ..., 0.6240, 0.5761, 0.5537],\n",
       "          [0.4226, 0.3697, 0.5057,  ..., 0.4217, 0.3273, 0.4492],\n",
       "          ...,\n",
       "          [0.4473, 0.3959, 0.5591,  ..., 0.5774, 0.6899, 0.5551],\n",
       "          [0.5103, 0.4078, 0.5093,  ..., 0.4822, 0.5121, 0.3232],\n",
       "          [0.4195, 0.4698, 0.3745,  ..., 0.4251, 0.4534, 0.4336]],\n",
       "\n",
       "         [[0.6021, 0.6375, 0.6443,  ..., 0.6222, 0.6284, 0.6007],\n",
       "          [0.6475, 0.6185, 0.6016,  ..., 0.6912, 0.6463, 0.5675],\n",
       "          [0.6300, 0.6354, 0.6002,  ..., 0.6280, 0.6154, 0.5745],\n",
       "          ...,\n",
       "          [0.6794, 0.6235, 0.6235,  ..., 0.6334, 0.6411, 0.5583],\n",
       "          [0.6002, 0.6048, 0.6158,  ..., 0.6240, 0.5980, 0.5812],\n",
       "          [0.6377, 0.6573, 0.6473,  ..., 0.6389, 0.6554, 0.5626]],\n",
       "\n",
       "         [[0.5310, 0.5871, 0.5409,  ..., 0.5154, 0.5274, 0.5401],\n",
       "          [0.5665, 0.5458, 0.5855,  ..., 0.5958, 0.5676, 0.4852],\n",
       "          [0.5913, 0.6158, 0.5120,  ..., 0.5843, 0.5677, 0.5443],\n",
       "          ...,\n",
       "          [0.6260, 0.6249, 0.5443,  ..., 0.5447, 0.5349, 0.4859],\n",
       "          [0.5475, 0.6047, 0.5205,  ..., 0.5895, 0.5233, 0.5069],\n",
       "          [0.5529, 0.5936, 0.6059,  ..., 0.5744, 0.6101, 0.5079]]],\n",
       "\n",
       "\n",
       "        [[[0.4603, 0.3869, 0.4108,  ..., 0.3381, 0.3772, 0.4422],\n",
       "          [0.4932, 0.4709, 0.5890,  ..., 0.5441, 0.5878, 0.5077],\n",
       "          [0.4315, 0.3705, 0.4081,  ..., 0.4183, 0.3515, 0.4887],\n",
       "          ...,\n",
       "          [0.4543, 0.3633, 0.5591,  ..., 0.2868, 0.5540, 0.4997],\n",
       "          [0.5130, 0.4229, 0.5342,  ..., 0.5131, 0.5090, 0.4498],\n",
       "          [0.3890, 0.4425, 0.4519,  ..., 0.6433, 0.4853, 0.4370]],\n",
       "\n",
       "         [[0.6109, 0.6249, 0.6216,  ..., 0.5876, 0.5881, 0.6116],\n",
       "          [0.6470, 0.6476, 0.6311,  ..., 0.6181, 0.6856, 0.5794],\n",
       "          [0.5934, 0.6652, 0.5920,  ..., 0.5833, 0.6182, 0.5757],\n",
       "          ...,\n",
       "          [0.6451, 0.6463, 0.6401,  ..., 0.6891, 0.6277, 0.5386],\n",
       "          [0.6057, 0.5518, 0.6179,  ..., 0.6361, 0.6095, 0.5870],\n",
       "          [0.6572, 0.6266, 0.6529,  ..., 0.6134, 0.5809, 0.5808]],\n",
       "\n",
       "         [[0.5305, 0.5822, 0.5272,  ..., 0.4974, 0.4768, 0.5504],\n",
       "          [0.5659, 0.5738, 0.5543,  ..., 0.5566, 0.5844, 0.5006],\n",
       "          [0.5502, 0.6542, 0.5274,  ..., 0.5175, 0.5523, 0.4859],\n",
       "          ...,\n",
       "          [0.5937, 0.6670, 0.5356,  ..., 0.6836, 0.5593, 0.4667],\n",
       "          [0.5166, 0.5499, 0.5075,  ..., 0.6309, 0.5293, 0.4837],\n",
       "          [0.5874, 0.5344, 0.6076,  ..., 0.4915, 0.5212, 0.4862]]],\n",
       "\n",
       "\n",
       "        [[[0.4375, 0.3171, 0.3135,  ..., 0.3549, 0.4525, 0.4394],\n",
       "          [0.4822, 0.4672, 0.6191,  ..., 0.5146, 0.6320, 0.5464],\n",
       "          [0.4680, 0.4440, 0.5250,  ..., 0.3496, 0.3720, 0.4528],\n",
       "          ...,\n",
       "          [0.4288, 0.3368, 0.5674,  ..., 0.3746, 0.4910, 0.5676],\n",
       "          [0.5151, 0.3897, 0.4029,  ..., 0.4071, 0.5695, 0.3853],\n",
       "          [0.3884, 0.4844, 0.4681,  ..., 0.4446, 0.5103, 0.5027]],\n",
       "\n",
       "         [[0.5985, 0.6515, 0.6110,  ..., 0.6313, 0.5898, 0.5787],\n",
       "          [0.6602, 0.6510, 0.6173,  ..., 0.6048, 0.6802, 0.6128],\n",
       "          [0.5912, 0.6246, 0.6053,  ..., 0.5570, 0.6207, 0.5666],\n",
       "          ...,\n",
       "          [0.6935, 0.6779, 0.6192,  ..., 0.6863, 0.6346, 0.5767],\n",
       "          [0.6024, 0.6058, 0.6823,  ..., 0.6236, 0.5578, 0.5943],\n",
       "          [0.6285, 0.6568, 0.6507,  ..., 0.6273, 0.6424, 0.5896]],\n",
       "\n",
       "         [[0.5353, 0.6263, 0.5277,  ..., 0.5260, 0.5080, 0.5530],\n",
       "          [0.5612, 0.6222, 0.6092,  ..., 0.5491, 0.5617, 0.5137],\n",
       "          [0.5460, 0.5480, 0.4428,  ..., 0.5526, 0.5213, 0.4651],\n",
       "          ...,\n",
       "          [0.6354, 0.6750, 0.4486,  ..., 0.6258, 0.6251, 0.5591],\n",
       "          [0.5686, 0.6675, 0.5887,  ..., 0.5638, 0.4388, 0.4894],\n",
       "          [0.5517, 0.6009, 0.6302,  ..., 0.5768, 0.5705, 0.4983]]],\n",
       "\n",
       "\n",
       "        ...,\n",
       "\n",
       "\n",
       "        [[[0.3910, 0.2999, 0.3471,  ..., 0.3216, 0.3587, 0.4668],\n",
       "          [0.4775, 0.4540, 0.6797,  ..., 0.6280, 0.7105, 0.5499],\n",
       "          [0.4401, 0.5103, 0.4161,  ..., 0.2178, 0.3054, 0.3283],\n",
       "          ...,\n",
       "          [0.4658, 0.4501, 0.5534,  ..., 0.4164, 0.6013, 0.5418],\n",
       "          [0.4467, 0.3875, 0.5042,  ..., 0.3651, 0.4959, 0.3950],\n",
       "          [0.4292, 0.4608, 0.4397,  ..., 0.4721, 0.5502, 0.4825]],\n",
       "\n",
       "         [[0.5900, 0.6355, 0.6021,  ..., 0.6307, 0.5827, 0.6249],\n",
       "          [0.6895, 0.6451, 0.6171,  ..., 0.6421, 0.6174, 0.5646],\n",
       "          [0.6205, 0.5702, 0.6205,  ..., 0.6254, 0.6576, 0.5912],\n",
       "          ...,\n",
       "          [0.6936, 0.6009, 0.6407,  ..., 0.6430, 0.6252, 0.6125],\n",
       "          [0.6517, 0.5681, 0.6736,  ..., 0.5721, 0.6501, 0.5438],\n",
       "          [0.6442, 0.6399, 0.6612,  ..., 0.6234, 0.6304, 0.5365]],\n",
       "\n",
       "         [[0.5292, 0.5874, 0.4931,  ..., 0.6012, 0.5526, 0.5469],\n",
       "          [0.6147, 0.6179, 0.5304,  ..., 0.5537, 0.5078, 0.5046],\n",
       "          [0.5873, 0.4899, 0.5195,  ..., 0.6127, 0.5227, 0.5407],\n",
       "          ...,\n",
       "          [0.6059, 0.5976, 0.5344,  ..., 0.5754, 0.5922, 0.4880],\n",
       "          [0.6345, 0.6259, 0.5849,  ..., 0.5675, 0.5523, 0.5189],\n",
       "          [0.5756, 0.5754, 0.5998,  ..., 0.5258, 0.5400, 0.4648]]],\n",
       "\n",
       "\n",
       "        [[[0.4107, 0.2208, 0.3543,  ..., 0.4726, 0.3744, 0.4872],\n",
       "          [0.5421, 0.4611, 0.6231,  ..., 0.5277, 0.5506, 0.5366],\n",
       "          [0.4998, 0.3255, 0.4358,  ..., 0.3168, 0.4982, 0.4992],\n",
       "          ...,\n",
       "          [0.3767, 0.4999, 0.4869,  ..., 0.3794, 0.5061, 0.5534],\n",
       "          [0.5831, 0.4372, 0.5251,  ..., 0.2698, 0.4371, 0.4054],\n",
       "          [0.4399, 0.3941, 0.4325,  ..., 0.5972, 0.4996, 0.5241]],\n",
       "\n",
       "         [[0.6037, 0.6569, 0.5519,  ..., 0.6171, 0.6036, 0.5951],\n",
       "          [0.6178, 0.6713, 0.6249,  ..., 0.6318, 0.6934, 0.5766],\n",
       "          [0.5394, 0.6411, 0.6158,  ..., 0.5744, 0.6492, 0.6269],\n",
       "          ...,\n",
       "          [0.7175, 0.5845, 0.6199,  ..., 0.6278, 0.6942, 0.5922],\n",
       "          [0.6228, 0.5799, 0.5998,  ..., 0.5717, 0.6527, 0.5450],\n",
       "          [0.6628, 0.6218, 0.6356,  ..., 0.6003, 0.6095, 0.5763]],\n",
       "\n",
       "         [[0.5233, 0.6491, 0.4552,  ..., 0.5366, 0.5686, 0.5522],\n",
       "          [0.5398, 0.6455, 0.5383,  ..., 0.5324, 0.6070, 0.4746],\n",
       "          [0.4367, 0.5943, 0.4916,  ..., 0.5041, 0.5525, 0.5384],\n",
       "          ...,\n",
       "          [0.6700, 0.5011, 0.5943,  ..., 0.5931, 0.6884, 0.5465],\n",
       "          [0.5575, 0.5583, 0.4963,  ..., 0.5306, 0.5230, 0.4912],\n",
       "          [0.5832, 0.5786, 0.5747,  ..., 0.5215, 0.5672, 0.4694]]],\n",
       "\n",
       "\n",
       "        [[[0.4364, 0.3641, 0.3424,  ..., 0.4037, 0.3372, 0.4396],\n",
       "          [0.4877, 0.4678, 0.5689,  ..., 0.6010, 0.5881, 0.5360],\n",
       "          [0.4736, 0.4092, 0.4531,  ..., 0.3644, 0.5050, 0.3560],\n",
       "          ...,\n",
       "          [0.4695, 0.2693, 0.5095,  ..., 0.4983, 0.5061, 0.5508],\n",
       "          [0.5082, 0.4302, 0.5886,  ..., 0.4963, 0.4759, 0.3981],\n",
       "          [0.3858, 0.4792, 0.4487,  ..., 0.6000, 0.4346, 0.4479]],\n",
       "\n",
       "         [[0.5844, 0.6485, 0.5987,  ..., 0.6078, 0.5931, 0.5831],\n",
       "          [0.6156, 0.6688, 0.6204,  ..., 0.6007, 0.6356, 0.5687],\n",
       "          [0.5686, 0.6648, 0.6377,  ..., 0.5996, 0.6277, 0.5982],\n",
       "          ...,\n",
       "          [0.6956, 0.6390, 0.6964,  ..., 0.6359, 0.6415, 0.5491],\n",
       "          [0.6068, 0.6030, 0.6522,  ..., 0.5717, 0.6423, 0.5888],\n",
       "          [0.6515, 0.6430, 0.6236,  ..., 0.6242, 0.6488, 0.5719]],\n",
       "\n",
       "         [[0.5314, 0.6055, 0.5232,  ..., 0.5856, 0.5349, 0.5130],\n",
       "          [0.5136, 0.6162, 0.5618,  ..., 0.6215, 0.6313, 0.4937],\n",
       "          [0.4842, 0.6021, 0.4848,  ..., 0.4849, 0.4718, 0.5196],\n",
       "          ...,\n",
       "          [0.6059, 0.6477, 0.6014,  ..., 0.5365, 0.4993, 0.4434],\n",
       "          [0.5823, 0.6492, 0.5422,  ..., 0.5069, 0.5670, 0.4907],\n",
       "          [0.5744, 0.5815, 0.5697,  ..., 0.5176, 0.6148, 0.5271]]]],\n",
       "       grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class down_conv(nn.Module):\n",
    "    \"\"\" Down convolution with a kernel size of 4x4 and optional batch normalization.\n",
    "    Args:\n",
    "    in_c : int\n",
    "    out_c: int\n",
    "    stride: int\n",
    "    batch_normalization: bool\n",
    "    \"\"\"\n",
    "    def __init__(self, in_c, out_c, kernel_size, stride, padding, batch_normalization=True):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv= nn.Conv2d(in_c, out_c, kernel_size=kernel_size, stride=stride, padding=padding)\n",
    "        if batch_normalization:\n",
    "            self.bn= nn.BatchNorm2d(out_c)\n",
    "        else: \n",
    "            self.bn = None\n",
    "        self.relu= nn.LeakyReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x= self.conv(x)\n",
    "        if self.bn:\n",
    "            x= self.bn(x)\n",
    "        x= self.relu(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class up_conv(nn.Module):\n",
    "    def __init__(self, in_c, out_c):\n",
    "        super().__init__()\n",
    "        self.upsample= nn.ConvTranspose2d(in_c, out_c, kernel_size=2, stride=2, padding=0)\n",
    "        # self.conv= nn.Conv2d(in_c+in_c, out_c, kernel_size=3, stride=1)\n",
    "        self.conv= nn.Conv2d(out_c, out_c, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn= nn.BatchNorm2d(out_c)\n",
    "        self.relu= nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x= torch.cat([x, skip], axis=1)\n",
    "        x= self.upsample(x)\n",
    "        # print(x.shape)\n",
    "        x= self.conv(x)\n",
    "        x= self.bn(x)\n",
    "        x= self.relu(x)\n",
    "        return x\n",
    "\n",
    "class final_conv(nn.Module):\n",
    "    def __init__(self, in_c, out_c):\n",
    "        super().__init__()\n",
    "        self.upsample= nn.ConvTranspose2d(in_c, out_c, kernel_size=2, stride=2, padding=0)\n",
    "        self.conv= nn.Conv2d(out_c, out_c, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x= self.upsample(x)\n",
    "        x= self.conv(x)\n",
    "        # need to add sigmoid?\n",
    "        return x\n",
    "\n",
    "class residual_block(nn.Module):\n",
    "    def __init__(self, in_c):\n",
    "        super().__init__()\n",
    "        self.conv= nn.Sequential(\n",
    "            nn.Conv2d(in_c, in_c*2, kernel_size=1, bias=False),\n",
    "            # nn.BatchNorm2d(in_c*2),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.Conv2d(in_c*2, in_c*2, kernel_size=3, padding=1, bias=False),\n",
    "            # nn.BatchNorm2d(in_c*2),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.Conv2d(in_c*2, in_c, kernel_size=1, bias=False),\n",
    "            nn.LeakyReLU(inplace=True)\n",
    "        )\n",
    "    \n",
    "    def forward(self,x):\n",
    "        return x+self.conv(x)\n",
    "    \n",
    "class segmentor(nn.Module):\n",
    "    def __init__(self, num_classes, filter_size= [64, 128, 256, 512]):\n",
    "        super().__init__()\n",
    "        self.encoder1= down_conv(1, filter_size[0], 7, 2, 3, batch_normalization=False) # 1 -> 64\n",
    "        self.resblock_1= residual_block(filter_size[0])\n",
    "        self.encoder2= down_conv(filter_size[0], filter_size[1],5, 2, 2) # 64 -> 128\n",
    "        self.resblock_2= residual_block(filter_size[1])\n",
    "        self.encoder3= down_conv(filter_size[1], filter_size[2],5, 2, 2) # 128 -> 256\n",
    "        self.resblock_3= residual_block(filter_size[2])\n",
    "        self.encoder4= down_conv(filter_size[2], filter_size[3],3, 2, 1) # 256 -> 512\n",
    "        self.resblock_4= residual_block(filter_size[3])\n",
    "\n",
    "        self.decoder1= up_conv(filter_size[3], filter_size[2]) # 512 -> 256\n",
    "        self.dresblock_1= residual_block(filter_size[2])\n",
    "        self.decoder2= up_conv(filter_size[2] + filter_size[2], filter_size[1]) # (256+256) -> 128 (double the initial size because of concatenation)\n",
    "        self.dresblock_2= residual_block(filter_size[1])\n",
    "        self.decoder3= up_conv(filter_size[1] + filter_size[1], filter_size[0]) # (128+128) -> 64\n",
    "        self.dresblock_3= residual_block(filter_size[0])\n",
    "\n",
    "        self.final_conv= final_conv(filter_size[0] + filter_size[0], 3) # (64+64) -> 3\n",
    "\n",
    "        self.output= nn.Conv2d(3, num_classes, kernel_size=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        el1= self.encoder1(x)\n",
    "        el1= self.resblock_1(el1)\n",
    "        print(\"el1:\" ,el1.shape)\n",
    "        \n",
    "        el2= self.encoder2(el1)\n",
    "        el2= self.resblock_2(el2)\n",
    "        print(\"el2:\" ,el2.shape)\n",
    "\n",
    "        el3= self.encoder3(el2)\n",
    "        el3= self.resblock_3(el3)\n",
    "        print(\"el3:\" ,el3.shape)\n",
    "\n",
    "        el4= self.encoder4(el3)\n",
    "        el4= self.resblock_4(el4)\n",
    "        print(\"el4:\" ,el4.shape)\n",
    "\n",
    "        dl1= self.decoder1(el4)\n",
    "        dl1= self.dresblock_1(dl1)\n",
    "        print(\"dl1:\" ,dl1.shape)\n",
    "        # print(\"cat:\", torch.cat([dl1, el3], axis=1).shape)\n",
    "        dl2= self.decoder2(torch.cat([dl1, el3], axis=1))\n",
    "        dl2= self.dresblock_2(dl2)\n",
    "        print(\"dl2:\" ,dl2.shape)\n",
    "        \n",
    "        dl3= self.decoder3(torch.cat([dl2, el2], axis=1))\n",
    "        dl3= self.dresblock_3(dl3)\n",
    "        print(\"dl3:\" ,dl3.shape)\n",
    "\n",
    "        dl4= self.final_conv(torch.cat([dl3, el1], axis=1))\n",
    "\n",
    "        output= self.output(dl4)\n",
    "\n",
    "        return torch.sigmoid(output)\n",
    "\n",
    "class critic(nn.Module):\n",
    "    def __init__(self, input_c, filter_size=[64, 128, 256, 512]):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1= down_conv(input_c, filter_size[0], batch_normalization=False)\n",
    "        self.conv2= down_conv(filter_size[0], filter_size[1])\n",
    "        self.conv3= down_conv(filter_size[1], filter_size[2])\n",
    "        self.conv4= down_conv(filter_size[2], filter_size[3])\n",
    "\n",
    "        self.conv5= up_conv(filter_size[3], filter_size[2])\n",
    "        self.conv6= up_conv(filter_size[2], filter_size[1])\n",
    "        self.conv7= up_conv(filter_size[1], filter_size[0])\n",
    "\n",
    "        self.conv= final_conv(filter_size[0], input_c)\n",
    "\n",
    "\n",
    "    def forward(self, pred, true, ground_truth):\n",
    "        masked_pred= pred*true # masking the predicted image with the true image\n",
    "        masked_truth= ground_truth*true # masking the ground truth with the true image\n",
    "        c1_pred= self.conv1(masked_pred) # level 1\n",
    "        c2_pred= self.conv2(c1_pred) # level 2\n",
    "        c3_pred= self.conv3(c2_pred) # level 3\n",
    "\n",
    "        c1_gt= self.conv1(masked_truth)\n",
    "        c2_gt= self.conv2(c1_gt)\n",
    "        c3_gt= self.conv3(c2_gt)\n",
    "\n",
    "        c= loss(masked_pred, masked_truth)\n",
    "        c1= loss(c1_pred, c1_gt)\n",
    "        c2= loss(c2_pred, c2_gt)\n",
    "        c3= loss(c3_pred, c3_gt)    \n",
    "        # print(c.shape)\n",
    "        # print(c1.shape)\n",
    "        # print(c2.shape)\n",
    "        # print(c3.shape)\n",
    "\n",
    "\n",
    "        l_mae= torch.cat([c, c1, c2, c3], axis=1).mean(dim=1)\n",
    "\n",
    "        return l_mae\n",
    "    \n",
    "def loss(pred, target):\n",
    "    loss1= torch.abs(pred-target).mean(dim=[1,2,3])\n",
    "    return loss1.unsqueeze(dim=1)\n",
    "\n",
    "class GAN(pl.LightningModule):\n",
    "    def __init__(self, num_classes, lr=0.0002):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self.segmentor= segmentor(self.hparams.num_classes)\n",
    "        self.critic = critic(self.hparams.num_classes)\n",
    "        self.automatic_optimization = False\n",
    "        \n",
    "        self.validation_step_dice = []\n",
    "        self.validation_step_bce = []\n",
    "    \n",
    "    def forward(self, z):\n",
    "        return self.segmentor(z)\n",
    "    \n",
    "    def adversarial_loss(self, l_mae):\n",
    "        return l_mae.mean()\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "    \n",
    "        true_imgs, gt, _= batch\n",
    "        opt_c0, opt_s= self.optimizers()\n",
    "\n",
    "        # train the critic\n",
    "        pred_imgs= self(true_imgs).detach()\n",
    "        self.toggle_optimizer(opt_c0)\n",
    "        lmae_c0= self.critic(pred_imgs, true_imgs, gt)\n",
    "        c_loss= self.adversarial_loss(lmae_c0)\n",
    "        self.manual_backward(c_loss)\n",
    "        self.log(\"c0_loss\", c_loss, prog_bar=True)\n",
    "        opt_c0.step()\n",
    "        opt_c0.zero_grad()\n",
    "        self.untoggle_optimizer(opt_c0)\n",
    "\n",
    "        # train the segmentor\n",
    "        self.toggle_optimizer(opt_s)\n",
    "        pred_imgs= self(true_imgs)\n",
    "        lmae_s= self.critic(pred_imgs, true_imgs, gt)\n",
    "        s_loss= self.adversarial_loss(lmae_s)\n",
    "        # print(s_loss)\n",
    "        self.log(\"s_loss\", s_loss, prog_bar=True)\n",
    "        self.manual_backward(s_loss)\n",
    "        opt_s.step()\n",
    "        opt_s.zero_grad()\n",
    "        self.untoggle_optimizer(opt_s)\n",
    "\n",
    "\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        lr = self.hparams.lr\n",
    "\n",
    "        opt_s= torch.optim.Adam(self.segmentor.parameters(), lr=lr)\n",
    "        opt_c0= torch.optim.Adam(self.critic.parameters(), lr=lr)\n",
    "\n",
    "        return [opt_c0, opt_s], []\n",
    "\n",
    "    def validation_step(self, batch, batch_index):\n",
    "        true_imgs, gt, _= batch\n",
    "        pred_imgs=self(true_imgs)\n",
    "\n",
    "        dice_score= dice_loss(pred_imgs, gt)\n",
    "        bce_loss= nn.BCELoss()(pred_imgs, gt)\n",
    "\n",
    "        self.validation_step_dice.append(dice_score.item())\n",
    "        self.validation_step_bce.append(bce_loss.item())\n",
    "        # val_loss= self.adversarial_loss(val_lmae)\n",
    "    \n",
    "    def on_validation_epoch_end(self):\n",
    "        dice_score= np.mean(self.validation_step_dice)\n",
    "        bce_loss= np.mean(self.validation_step_bce)\n",
    "        print(\"\\nvalidation dice loss in epoch {}: {}\".format(self.current_epoch, dice_score))\n",
    "        print(\"validation bce loss in epoch {}: {}\".format(self.current_epoch, bce_loss))\n",
    "        self.log(\"dice loss\", dice_score)\n",
    "        self.log(\"bce\", bce_loss)\n",
    "        self.validation_step_dice.clear()\n",
    "        self.validation_step_bce.clear()\n",
    "        \n",
    "def dice_loss(pred, target, smooth = 1.):\n",
    "    pred = pred.contiguous()\n",
    "    target = target.contiguous()    \n",
    "\n",
    "    intersection = (pred * target).sum(dim=2).sum(dim=2)\n",
    "    # print(intersection.shape)\n",
    "    dice = 1-((2. * intersection + smooth) / (pred.sum(dim=2).sum(dim=2) + target.sum(dim=2).sum(dim=2) + smooth))\n",
    "    \n",
    "    return dice.mean()\n",
    "        \n",
    "x= torch.randn((8,2,20,20))\n",
    "y= torch.randn((8,1,128,128))\n",
    "z= torch.randn((8,1,228,228))\n",
    "\n",
    "# nn.Conv2d(4,8,1)(z).shape\n",
    "segmentor(3)(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 3 required positional arguments: 'kernel_size', 'stride', and 'padding'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model\u001b[39m=\u001b[39m GAN(num_classes\u001b[39m=\u001b[39;49m\u001b[39m3\u001b[39;49m, lr\u001b[39m=\u001b[39;49m \u001b[39m0.0002\u001b[39;49m)\n\u001b[1;32m      2\u001b[0m torch\u001b[39m.\u001b[39mmanual_seed(\u001b[39m2023\u001b[39m)\n\u001b[1;32m      3\u001b[0m trainer \u001b[39m=\u001b[39m pl\u001b[39m.\u001b[39mTrainer(max_epochs\u001b[39m=\u001b[39m\u001b[39m30\u001b[39m, devices\u001b[39m=\u001b[39m[\u001b[39m1\u001b[39m])\n",
      "Cell \u001b[0;32mIn[5], line 183\u001b[0m, in \u001b[0;36mGAN.__init__\u001b[0;34m(self, num_classes, lr)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msave_hyperparameters()\n\u001b[1;32m    182\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msegmentor\u001b[39m=\u001b[39m segmentor(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhparams\u001b[39m.\u001b[39mnum_classes)\n\u001b[0;32m--> 183\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcritic \u001b[39m=\u001b[39m critic(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhparams\u001b[39m.\u001b[39;49mnum_classes)\n\u001b[1;32m    184\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mautomatic_optimization \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalidation_step_dice \u001b[39m=\u001b[39m []\n",
      "Cell \u001b[0;32mIn[5], line 136\u001b[0m, in \u001b[0;36mcritic.__init__\u001b[0;34m(self, input_c, filter_size)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, input_c, filter_size\u001b[39m=\u001b[39m[\u001b[39m64\u001b[39m, \u001b[39m128\u001b[39m, \u001b[39m256\u001b[39m, \u001b[39m512\u001b[39m]):\n\u001b[1;32m    134\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()\n\u001b[0;32m--> 136\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv1\u001b[39m=\u001b[39m down_conv(input_c, filter_size[\u001b[39m0\u001b[39;49m], batch_normalization\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m    137\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv2\u001b[39m=\u001b[39m down_conv(filter_size[\u001b[39m0\u001b[39m], filter_size[\u001b[39m1\u001b[39m])\n\u001b[1;32m    138\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv3\u001b[39m=\u001b[39m down_conv(filter_size[\u001b[39m1\u001b[39m], filter_size[\u001b[39m2\u001b[39m])\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() missing 3 required positional arguments: 'kernel_size', 'stride', and 'padding'"
     ]
    }
   ],
   "source": [
    "model= GAN(num_classes=3, lr= 0.0002)\n",
    "torch.manual_seed(2023)\n",
    "trainer = pl.Trainer(max_epochs=30, devices=[1])\n",
    "early_stopping= EarlyStopping(monitor=\"dice loss\", mode= 'max', patience=3)\n",
    "trainer.fit(model, train_dataloaders=train_dataloader, val_dataloaders=val_dataloader)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### performance log\n",
    "\n",
    "epoch 17 \n",
    "dice= 0.559\n",
    "c_loss = 0.00059, s_loss= 0.00059\n",
    "\n",
    "version = 53 \\\n",
    "dice loss= 0.614 \\\n",
    "c_loss = 0.0028 s_loss= 0.0032 \n",
    "\n",
    "version = 57 \\\n",
    "dice= 0.7\\\n",
    "c_loss= 0.011, s_loss= 0.00367\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_checkpoint(checkpoint_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.load_from_checkpoint(checkpoint_path=checkpoint_path)\n",
    "model.cuda()\n",
    "# model= model.cuda()\n",
    "\n",
    "model.eval()\n",
    "\n",
    "test_loader= get_test_data(TRAIN_IMG_DIR, TRAIN_MASK_DIR, BATCH_SIZE, \n",
    "                           test_transform=None, \n",
    "                           num_workers=NUM_WORKERS, \n",
    "                           pin_memory=PIN_MEMORY)\n",
    "\n",
    "with Path(\"/home/haobo/HaoboSeg-pytorch/eizzaty/\"):\n",
    "    if not os.path.exists(model_category):\n",
    "        print(model_category + \" does not exist. Creating directory...\")\n",
    "        os.makedirs(model_category)\n",
    "        print(model_category+ \" created!\")\n",
    "\n",
    "    for j, (inputs, masks, file_names) in enumerate(test_loader):\n",
    "\n",
    "        inputs, masks= inputs.cuda(), masks.cuda()\n",
    "\n",
    "        pred= model(inputs)\n",
    "        pred= torch.sigmoid(pred)\n",
    "        pred = pred.cpu().detach().numpy()\n",
    "        ori_imgs = inputs.cpu().detach().numpy()\n",
    "        ground_truth = masks.cpu().detach().numpy()\n",
    "\n",
    "\n",
    "        if not os.path.exists(model_category+\"/predicted_images\"):\n",
    "            print(model_category+\"/predicted_images\" + \" does not exist. Creating directory...\")\n",
    "            os.makedirs(model_category+\"/predicted_images\")\n",
    "            print(model_category+\"/predicted_images\"+ \" created!\")\n",
    "        \n",
    "        if not os.path.exists(model_category+\"/original_images\"):\n",
    "            print(model_category+\"/original_images\" + \" does not exist. Creating directory...\")\n",
    "            os.makedirs(model_category+\"/original_images\")\n",
    "            print(model_category+\"/original_images\"+ \" created!\")\n",
    "\n",
    "        if not os.path.exists(model_category+\"/groundtruth_images\"):\n",
    "            print(model_category+\"/groundtruth_images\" + \" does not exist. Creating directory...\")\n",
    "            os.makedirs(model_category+\"/groundtruth_images\")\n",
    "            print(model_category+\"/groundtruth_images\"+ \" created!\")\n",
    "        \n",
    "\n",
    "        for i in range(inputs.size(0)):\n",
    "            ground_truth_final= np.argmax(np.transpose(ground_truth[i].reshape(3, IMAGE_HEIGHT, IMAGE_WIDTH), (1,2,0)), axis=2)\n",
    "            pred_final= np.argmax(np.transpose(pred[i].reshape(3, IMAGE_HEIGHT, IMAGE_WIDTH), (1,2,0)), axis=2)\n",
    "            cv2.imwrite(model_category+\"/groundtruth_images/\"+Path(file_names[i]).stem + \"_groundtruth.png\", 100*ground_truth_final)\n",
    "            cv2.imwrite(model_category+\"/predicted_images/\"+Path(file_names[i]).stem + \"_pred.png\", 100*pred_final)\n",
    "            cv2.imwrite(model_category+\"/original_images/\"+Path(file_names[i]).stem + \"_ori.png\", 255*ori_imgs[i].reshape(IMAGE_HEIGHT, IMAGE_WIDTH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
